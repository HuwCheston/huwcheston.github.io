---
title: "Jazz Trio Database: Automated Timing Annotation of Jazz Piano Trio Recordings Processed Using Audio Source Separation"
collection: publications
preprint: false
excerpt: 'We introduce the Jazz Trio Database, a dataset of 45 hours of jazz piano trio recordings with automatically generated annotations for every performer (piano soloist, bass and drums accompaniment) in the ensemble.'
date: 2024-08-27
venue: 'Transactions of the International Society for Music Information Retrieval'
paperurl: 'https://doi.org/10.5334/tismir.186'
imgurl: '/images/trio-database_img.png'
citation: 'Cheston H, Schlichting JL, Cross I, & Harrison PMC. Jazz Trio Database: Automated Annotation of Jazz Piano Trio Recordings Processed Using Audio Source Separation. <i>Transactions of the International Society for Music Information Retrieval</i>. 2024; 7(1), 144–158.'
---

<img src='/images/trio-database-explorer_img.png'>

[![Paper](http://img.shields.io/badge/Paper-doi/10.5334/tismir.186-blue)](https://doi.org/10.5334/tismir.186) <br>
[![Dataset](http://img.shields.io/badge/Dataset-Available_on_GitHub-purple)](https://github.com/HuwCheston/Jazz-Trio-Database) [![Documentation](http://img.shields.io/badge/Documentation-Available_on_GitHub-purple)](https://huwcheston.github.io/Jazz-Trio-Database/)

Recent advances in automatic music transcription have facilitated the creation of large databases of symbolic transcriptions of improvised music forms including jazz, where traditional notated scores are not normally available. In conjunction with music source separation models that enable audio to be “demixed” into separate signals for multiple instrument classes, these algorithms can also be applied to generate annotations for every musician in a performance. This can enable the analysis of interesting performer-level and ensemble-level features that have often been difficult to explore. To this end, we introduce Jazz Trio Database (JTD), a dataset of 44.5 h of jazz piano solos accompanied by bass and drums, with automatically generated annotations for each performer. These annotations consist of onset, beat, and downbeat timestamps, alongside MIDI for the piano soloist. Suitable recordings, broadly representative of the “straight-ahead” jazz style, were identified by scraping user-based listening and discographic data; source separation models were applied to isolate audio for each performer in the trio; annotations were generated by applying appropriate algorithms to both the separated and the mixed audio sources. Onset annotations generated by the pipeline achieved a mean F-measure of 0.94 when compared with ground truth annotations. We conduct several analyses of JTD, including with relation to swing and inter-performer synchronization. We anticipate that JTD will be useful in a variety of music information–retrieval tasks, including artist identification and expressive performance modeling. We have made JTD, including the annotations and associated source code, available at https://github.com/HuwCheston/Jazz-Trio-Database